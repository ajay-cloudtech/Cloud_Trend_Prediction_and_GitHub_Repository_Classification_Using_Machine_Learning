{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d721ce-b87c-4c76-87e2-a33e1314f640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:43:45 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset size: 80525 rows\n",
      "\n",
      "Testing with 100 repositories:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:43:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:43:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:43:46 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with 1000 repositories:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:43:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:43:46 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with 10000 repositories:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:43:47 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with 25000 repositories:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:43:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "18:43:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "18:43:47 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with 50000 repositories:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:43:47 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with 80525 repositories:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:43:48 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Results:\n",
      "                               label  latency  throughput  dataset_size   mae\n",
      "  Overall Cloud Projects (100 repos)     0.49      930.02           100  0.17\n",
      " Overall Cloud Projects (1000 repos)     0.28     3137.45          1000  0.78\n",
      "Overall Cloud Projects (10000 repos)     0.37     2983.38         10000  2.76\n",
      "Overall Cloud Projects (25000 repos)     0.40     2710.22         25000  5.19\n",
      "Overall Cloud Projects (50000 repos)     0.40     2734.28         50000  8.15\n",
      "Overall Cloud Projects (80525 repos)     0.44     2477.48         80525 11.41\n",
      "\n",
      "Saved performance metrics to 'prophet_performance_metrics.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "df = pd.read_csv(\"github_repos_filtered.csv\")\n",
    "df['created_at'] = pd.to_datetime(df['created_at'], format='mixed')\n",
    "full_size = len(df)\n",
    "print(f\"Full dataset size: {full_size} rows\")\n",
    "\n",
    "# Function to measure Prophet model performance\n",
    "def measure_performance(df, col_name=\"is_cloud_project\", label=\"Overall Cloud Projects\", periods=365):\n",
    "    start_time = time.time()\n",
    "    df_trend = df[df[col_name] == 1].groupby('created_at').size().reset_index(name='y')\n",
    "    df_trend.rename(columns={'created_at': 'ds'}, inplace=True)\n",
    "    if df_trend.empty:\n",
    "        print(f\"Skipping {label} - No data available.\")\n",
    "        return None\n",
    "    \n",
    "    # Tuned Prophet model\n",
    "    model = Prophet(\n",
    "        changepoint_prior_scale=0.1,  \n",
    "        yearly_seasonality=True,      \n",
    "        weekly_seasonality=True       \n",
    "    )\n",
    "    model.fit(df_trend)\n",
    "    future = model.make_future_dataframe(periods=periods)\n",
    "    forecast = model.predict(future)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    latency = end_time - start_time\n",
    "    throughput = len(forecast) / latency\n",
    "    dataset_size = len(df)\n",
    "    historical_forecast = forecast[forecast['ds'].isin(df_trend['ds'])]['yhat']\n",
    "    mae = mean_absolute_error(df_trend['y'], historical_forecast)\n",
    "    \n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"latency\": latency,\n",
    "        \"throughput\": throughput,\n",
    "        \"dataset_size\": dataset_size,\n",
    "        \"mae\": mae\n",
    "    }\n",
    "# Define dataset sizes for performance testing\n",
    "sizes = [100, 1000, 10000, 25000, 50000, full_size]\n",
    "results = []\n",
    "# Test performance across different dataset sizes\n",
    "for size in sizes:\n",
    "    print(f\"\\nTesting with {size} repositories:\")\n",
    "    if size <= full_size:\n",
    "        sampled_df = df.sample(n=size, random_state=42)\n",
    "    else:\n",
    "        repeat_factor = (size // full_size) + 1\n",
    "        sampled_df = pd.concat([df] * repeat_factor, ignore_index=True).iloc[:size]\n",
    "        sampled_df['created_at'] = pd.to_datetime(sampled_df['created_at'], format='mixed')\n",
    "    result = measure_performance(sampled_df, \"is_cloud_project\", f\"Overall Cloud Projects ({size} repos)\")\n",
    "    if result:\n",
    "        results.append(result)\n",
    "# display results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['latency'] = results_df['latency'].round(2)\n",
    "results_df['throughput'] = results_df['throughput'].round(2)\n",
    "results_df['mae'] = results_df['mae'].round(2)\n",
    "print(\"\\nPerformance Results:\")\n",
    "print(results_df.to_string(index=False))\n",
    "results_df.to_csv(\"prophet_performance_metrics.csv\", index=False)\n",
    "print(\"\\nSaved performance metrics to 'prophet_performance_metrics.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea2955f-651f-44b0-b9e4-94aed06adae9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
